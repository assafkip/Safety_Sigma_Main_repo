"""
Safety Sigma LLM vs Scripted Parity Tests

Verifies that both LLM and scripted pipelines preserve critical indicators exactly.
Tests for golden indicator parity: "$1,998.88", "VOID 2000", "wa.me/123456789"
"""

import json
import pytest
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Set, Any

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class TestParityVsScripted:
    """Test parity between LLM and scripted pipelines"""
    
    # Critical indicators that must be preserved in both pipelines
    GOLDEN_INDICATORS = [
        "$1,998.88",
        "VOID 2000", 
        "wa.me/123456789"
    ]
    
    def setup_method(self):
        """Set up test fixtures"""
        self.project_root = project_root
        self.artifacts_dir = self.project_root / "artifacts"
        
        # Paths for comparison
        self.scripted_rules_path = self.artifacts_dir / "demo_rules.json"
        self.llm_output_dir = self.artifacts_dir / "llm_output"
        self.llm_ir_path = self.llm_output_dir / "ir.json"
        self.llm_rules_path = self.llm_output_dir / "rules"
    
    def load_scripted_artifacts(self) -> Dict[str, Any]:
        """Load artifacts from scripted pipeline"""
        if not self.scripted_rules_path.exists():
            # Try to generate scripted artifacts
            try:
                self._generate_scripted_artifacts()
            except Exception as e:
                pytest.skip(f"Cannot generate scripted artifacts: {e}")
        
        if not self.scripted_rules_path.exists():
            pytest.skip("Scripted artifacts not available")
            
        with open(self.scripted_rules_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def load_llm_artifacts(self) -> Dict[str, Any]:
        """Load artifacts from LLM pipeline"""
        if not self.llm_ir_path.exists():
            # Try to generate LLM artifacts
            try:
                self._generate_llm_artifacts()
            except Exception as e:
                pytest.skip(f"Cannot generate LLM artifacts: {e}")
        
        if not self.llm_ir_path.exists():
            pytest.skip("LLM artifacts not available")
            
        with open(self.llm_ir_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _generate_scripted_artifacts(self):
        """Generate scripted pipeline artifacts for testing"""
        atlas_pdf = self.project_root / "Reports" / "atlas-highlights-scams-and-fraud.pdf"
        if not atlas_pdf.exists():
            raise FileNotFoundError(f"Atlas PDF not found: {atlas_pdf}")
        
        cmd = [
            sys.executable, 
            str(self.project_root / "scripts" / "demo_pdf_to_rules.py"),
            "--pdf", str(atlas_pdf),
            "--json-out", str(self.scripted_rules_path),
            "--html-out", str(self.artifacts_dir / "demo_report.html")
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=self.project_root)
        if result.returncode != 0:
            raise RuntimeError(f"Scripted pipeline failed: {result.stderr}")
    
    def _generate_llm_artifacts(self):
        """Generate LLM pipeline artifacts for testing"""
        atlas_pdf = self.project_root / "Reports" / "atlas-highlights-scams-and-fraud.pdf"
        config_path = self.project_root / "configs" / "llm_dev.yaml"
        
        if not atlas_pdf.exists():
            raise FileNotFoundError(f"Atlas PDF not found: {atlas_pdf}")
        if not config_path.exists():
            raise FileNotFoundError(f"LLM config not found: {config_path}")
        
        cmd = [
            sys.executable,
            str(self.project_root / "scripts" / "run_llm_pipeline.py"),
            "--doc", str(atlas_pdf),
            "--config", str(config_path),
            "--out", str(self.llm_output_dir)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=self.project_root)
        if result.returncode != 0:
            # LLM pipeline might fail in CI without API keys, but we can still test structure
            print(f"LLM pipeline warning: {result.stderr}")
    
    def extract_indicators_from_scripted(self, scripted_data: Dict[str, Any]) -> Set[str]:
        """Extract indicator values from scripted rules JSON"""
        indicators = set()
        
        # Extract from IR section
        ir_section = scripted_data.get("ir", {})
        if "indicators" in ir_section:
            for indicator in ir_section["indicators"]:
                if "verbatim" in indicator:
                    indicators.add(indicator["verbatim"])
                elif "literal" in indicator:
                    indicators.add(indicator["literal"])
        
        # Extract from JSON section
        json_section = scripted_data.get("json", {})
        if "indicators" in json_section:
            for indicator in json_section["indicators"]:
                if "verbatim" in indicator:
                    indicators.add(indicator["verbatim"])
                elif "literal" in indicator:
                    indicators.add(indicator["literal"])
        
        return indicators
    
    def extract_indicators_from_llm_ir(self, llm_ir: Dict[str, Any]) -> Set[str]:
        """Extract indicator values from LLM IR JSON"""
        indicators = set()
        
        extractions = llm_ir.get("extractions", [])
        for extraction in extractions:
            if "value" in extraction:
                indicators.add(extraction["value"])
        
        return indicators
    
    def extract_indicators_from_llm_rules(self) -> Set[str]:
        """Extract indicators from LLM rules files"""
        indicators = set()
        
        # Check regex rules
        regex_rules_path = self.llm_rules_path / "rules.regex"
        if regex_rules_path.exists():
            content = regex_rules_path.read_text(encoding='utf-8')
            # Extract patterns from regex rules (simplified)
            for golden in self.GOLDEN_INDICATORS:
                if golden in content or golden.replace(".", "\\.") in content:
                    indicators.add(golden)
        
        # Check JSON rules
        json_rules_path = self.llm_rules_path / "rules.json"
        if json_rules_path.exists():
            with open(json_rules_path, 'r', encoding='utf-8') as f:
                rules_data = json.load(f)
            
            rules = rules_data.get("rules", [])
            for rule in rules:
                if "pattern" in rule:
                    indicators.add(rule["pattern"])
        
        return indicators
    
    def test_golden_indicators_in_both_ir(self):
        """Test that golden indicators appear in both scripted and LLM IR"""
        scripted_data = self.load_scripted_artifacts()
        llm_ir = self.load_llm_artifacts()
        
        scripted_indicators = self.extract_indicators_from_scripted(scripted_data)
        llm_indicators = self.extract_indicators_from_llm_ir(llm_ir)
        
        print(f"Scripted indicators: {scripted_indicators}")
        print(f"LLM indicators: {llm_indicators}")
        
        # Test each golden indicator
        for golden in self.GOLDEN_INDICATORS:
            # Check if present in at least one place in each pipeline
            scripted_has = golden in scripted_indicators
            llm_has = golden in llm_indicators
            
            # If both pipelines found indicators, they should both have the golden ones
            if scripted_indicators and llm_indicators:
                assert scripted_has, f"Golden indicator '{golden}' missing from scripted pipeline"
                assert llm_has, f"Golden indicator '{golden}' missing from LLM pipeline"
            else:
                # If one pipeline has no indicators, just warn
                if scripted_indicators and not scripted_has:
                    pytest.fail(f"Golden indicator '{golden}' missing from scripted pipeline")
                if llm_indicators and not llm_has:
                    pytest.fail(f"Golden indicator '{golden}' missing from LLM pipeline")
    
    def test_golden_indicators_in_rules(self):
        """Test that golden indicators appear in generated rules from both pipelines"""
        scripted_data = self.load_scripted_artifacts()
        
        # Extract from scripted rules
        scripted_rules_indicators = set()
        
        # Check regex section
        if "regex" in scripted_data and "rules" in scripted_data["regex"]:
            for rule in scripted_data["regex"]["rules"]:
                if "pattern" in rule:
                    scripted_rules_indicators.add(rule["pattern"])
        
        # Extract from LLM rules
        llm_rules_indicators = self.extract_indicators_from_llm_rules()
        
        print(f"Scripted rules indicators: {scripted_rules_indicators}")
        print(f"LLM rules indicators: {llm_rules_indicators}")
        
        # Test each golden indicator in rules
        for golden in self.GOLDEN_INDICATORS:
            if scripted_rules_indicators:
                # Check if golden indicator appears in scripted rules (exact or escaped)
                scripted_has = (golden in scripted_rules_indicators or
                              any(golden in rule for rule in scripted_rules_indicators))
                
                if scripted_has and llm_rules_indicators:
                    # If scripted has it and LLM has rules, LLM should have it too
                    llm_has = golden in llm_rules_indicators
                    assert llm_has, f"Golden indicator '{golden}' in scripted rules but not LLM rules"
    
    def test_both_pipelines_produce_valid_json(self):
        """Test that both pipelines produce valid, parseable JSON"""
        # Test scripted output
        scripted_data = self.load_scripted_artifacts()
        assert isinstance(scripted_data, dict), "Scripted output should be a valid dict"
        assert "source" in scripted_data, "Scripted output should have 'source' field"
        
        # Test LLM IR output  
        llm_ir = self.load_llm_artifacts()
        assert isinstance(llm_ir, dict), "LLM IR should be a valid dict"
        assert "schema_version" in llm_ir, "LLM IR should have 'schema_version' field"
        assert "extractions" in llm_ir, "LLM IR should have 'extractions' field"
        assert isinstance(llm_ir["extractions"], list), "LLM extractions should be a list"
    
    def test_both_pipelines_have_provenance(self):
        """Test that both pipelines include provenance information"""
        scripted_data = self.load_scripted_artifacts()
        llm_ir = self.load_llm_artifacts()
        
        # Check scripted provenance
        if "json" in scripted_data and "indicators" in scripted_data["json"]:
            for indicator in scripted_data["json"]["indicators"]:
                if "source_span" in indicator:
                    assert "category_id" in indicator["source_span"], "Scripted indicators should have category_id in source_span"
                    assert "span_id" in indicator["source_span"], "Scripted indicators should have span_id in source_span"
        
        # Check LLM provenance
        extractions = llm_ir.get("extractions", [])
        for extraction in extractions:
            assert "provenance" in extraction, f"LLM extraction {extraction.get('id', 'unknown')} should have provenance"
            provenance = extraction["provenance"]
            assert "page" in provenance, "LLM provenance should have 'page'"
            assert "start" in provenance, "LLM provenance should have 'start'"
            assert "end" in provenance, "LLM provenance should have 'end'"
    
    def test_indicator_count_reasonable(self):
        """Test that both pipelines extract a reasonable number of indicators"""
        scripted_data = self.load_scripted_artifacts()
        llm_ir = self.load_llm_artifacts()
        
        # Count scripted indicators
        scripted_count = 0
        if "json" in scripted_data and "indicators" in scripted_data["json"]:
            scripted_count = len(scripted_data["json"]["indicators"])
        
        # Count LLM indicators
        llm_count = len(llm_ir.get("extractions", []))
        
        print(f"Scripted indicators: {scripted_count}")
        print(f"LLM indicators: {llm_count}")
        
        # Both should extract at least some indicators if they processed the PDF
        if scripted_count > 0 or llm_count > 0:
            # If one found indicators, both should find at least one
            # (allowing for different extraction approaches)
            assert scripted_count >= 0, "Scripted pipeline should extract indicators"
            assert llm_count >= 0, "LLM pipeline should extract indicators"


# Helper functions for standalone testing
def run_parity_check():
    """Standalone function to run parity check"""
    test_instance = TestParityVsScripted()
    test_instance.setup_method()
    
    try:
        print("Testing golden indicators in IR...")
        test_instance.test_golden_indicators_in_both_ir()
        print("âœ… Golden indicators IR test passed")
        
        print("Testing golden indicators in rules...")
        test_instance.test_golden_indicators_in_rules()
        print("âœ… Golden indicators rules test passed")
        
        print("Testing JSON validity...")
        test_instance.test_both_pipelines_produce_valid_json()
        print("âœ… JSON validity test passed")
        
        print("Testing provenance...")
        test_instance.test_both_pipelines_have_provenance()
        print("âœ… Provenance test passed")
        
        print("Testing indicator counts...")
        test_instance.test_indicator_count_reasonable()
        print("âœ… Indicator count test passed")
        
        print("ğŸ‰ All parity tests passed!")
        
    except Exception as e:
        print(f"âŒ Parity test failed: {e}")
        return False
    
    return True


if __name__ == "__main__":
    # Allow running parity check standalone
    run_parity_check()